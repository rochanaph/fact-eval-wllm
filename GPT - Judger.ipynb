{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94bbf1bc-7373-480f-8cf5-e981503dd231",
   "metadata": {},
   "source": [
    "# LLM Watermark Judger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bf9eb3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Load necessary packages and initialize OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd394d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----- SYSTEM PACKAGES ----- ##\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "## ----- IMPORT FUNCTIONS ----- ##\n",
    "sys.path.insert(0, os.getcwd())\n",
    "from judgerfunctions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa698589-4266-4ea0-9347-d36d73b6e2b3",
   "metadata": {},
   "source": [
    "## Judge New Samples\n",
    "\n",
    "Load sample pairs from a JSON file and use the GPT-judger to evaluate each sample, saving the results to the same data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb377e83",
   "metadata": {},
   "outputs": [],
   "source": "## Judge New Samples\n## Load sample pairs from a pickle file and use the GPT-judger to evaluate each sample, saving the results to JSON.\n\nfrom judgerfunctions import *\nimport pickle\nfrom evaluation.dataset import ClinicDataset\n\n# Configuration variables - update these for different experiments\nalgorithm = \"DIP\"                  # Algorithm: \"EXPEdit\", \"SWEET\", \"KGW\", etc.\nmodel = \"jsl\"                     # Model: \"meditron\", \"biogpt\", \"opt\", etc.\ndataset = \"HQA2\"                        # Dataset: \"MEQS\", \"HQA\", \"HQA2\", etc.\ngamma = 0.5                           # Optional parameter for some algorithms\ndelta = 2                           # Optional parameter for some algorithms\nentropy = 0.9                         # Optional parameter for some algorithms\nnlength = 256                          # Sequence length parameter for EXPEdit\n\n# Determine task type based on dataset for appropriate judger prompt\nif dataset in ['HQA']:\n    task_type = 'GEN'  # Text completion/generation task\nelif dataset in ['HQA2']:\n    task_type = 'QA'   # Question-answering task\nelif dataset in ['MEQS']:\n    task_type = 'SUMM' # Summarization task\nelse:\n    task_type = 'QA'   # Default to QA task\n\nprint(f\"Using task type: {task_type} for dataset: {dataset}\")\n\n# Handle dataset paths similar to generate_simple.ipynb\nif dataset == 'HQA':\n    dataset_path = \"dataset/hqa/hqa_processed_230words.json\"\nelif dataset == 'HQA2':\n    dataset_path = \"dataset/hqa2/hqa2_processed_4.json\"\nelif dataset == 'MEQS':\n    dataset_path = \"dataset/meqsum/meqsum_processed.json\"\n\n# Build the pickle file path based on algorithm parameters\ndump_path = f\"logs/{algorithm}\"\nif algorithm == \"KGW\" and gamma is not None and delta is not None:\n    pickle_name = f'{algorithm}-g{gamma}-d{delta}-{model}-{dataset}.pkl'\nelif algorithm == \"SWEET\" and entropy is not None and gamma is not None and delta is not None:\n    pickle_name = f'{algorithm}-e{entropy}-g{gamma}-d{delta}-{model}-{dataset}.pkl'\nelif algorithm == \"EXPEdit\" and nlength is not None:\n    pickle_name = f'{algorithm}-n{nlength}-{model}-{dataset}.pkl'\nelif algorithm == \"DIP\":\n    pickle_name = f'{algorithm}-a0.45-{model}-{dataset}.pkl'\nelse:\n    raise ValueError(f\"Incomplete parameters for algorithm: {algorithm}\")\npickle_path = os.path.join(dump_path, pickle_name)\n\nprint(f\"Using pickle file: {pickle_path}\")\nprint(f\"Using dataset: {dataset_path}\")\n\n# Load dataset to get prompts\nmy_dataset = ClinicDataset(dataset_path)\n\n# Load the watermarked, unwatermarked, and natural texts\nwith open(pickle_path, 'rb') as f:\n    watermarked_texts, unwatermarked_texts, natural_texts = pickle.load(f)\n # NOTE (May 2): If the algorithm is EXPEdit and dataset is MEQS, we need to load the KGW data instead for unwatermarked texts\nif algorithm == \"EXPEdit\" and dataset == \"MEQS\":\n    temp_pickle_path = pickle_path.replace(\"EXPEdit\", \"KGW\")\n    temp_pickle_path = temp_pickle_path.replace(f\"-n256\", \"-g0.5-d2\")\n    with open(temp_pickle_path, 'rb') as f:\n        _, unwatermarked_texts, _ = pickle.load(f)\n\n# Create output filename based on experiment parameters\noutput_filename = f\"results_NEW2_judged_{pickle_name}.json\"\n\n# Initialize results list\ndata_list = []\n\n# Create data structure for judging\nfor i in range(len(watermarked_texts)):\n    prompt = my_dataset.get_prompt(i)\n    item = {\n        \"sample_id\": i,\n        \"prompt\": prompt, \n        \"w_output\": watermarked_texts[i][len(prompt):],\n        \"uw_output\": unwatermarked_texts[i][len(prompt):],\n        \"task_type\": task_type  # Add task type to the data structure\n    }\n    data_list.append(item)\n\n# Judge the samples\nfor i in range(len(data_list)):\n    item = data_list[i]\n\n    # Unpack needed fields\n    prompt = item[\"prompt\"]\n    uw_output = item[\"uw_output\"]\n    w_output = item[\"w_output\"]\n    current_task_type = item[\"task_type\"]\n\n    # Evaluate responses using gpt with appropriate task type\n    judge_choice = \"\"\n    exception_counter = 0\n    max_retries = 3\n    while judge_choice == \"\" or judge_choice == \"Model Failure\":\n        try:\n            t1 = time.time()\n            judge_output, judge_choice, scores_U, scores_W, is_randomized = gpt_judge(\n                prompt, uw_output, w_output, task_type=current_task_type\n            )\n            t2 = time.time()\n            break\n        except Exception as e:\n            exception_counter += 1\n            print(f\"\\nError with sample {i}: {e}\")\n            if exception_counter >= max_retries:\n                judge_output = \"\"\n                judge_choice = \"Error\"\n                scores_U = []\n                scores_W = []\n                is_randomized = None\n                break\n            time.sleep(2)  # Wait before retrying\n\n    # Save results\n    item[\"judge_output\"] = judge_output\n    item[\"randomized\"] = is_randomized\n    item[\"judge_choice\"] = judge_choice\n    item[\"scores_U\"] = scores_U\n    item[\"scores_W\"] = scores_W\n    item[\"evaluation_time\"] = round(t2 - t1, 3) if judge_choice != \"Error\" else 0\n\n    print(f\"{i+1} out of {len(data_list)} items processed! Evaluation time: {item['evaluation_time']}s\", end=\"\\r\")\n    \n    # Save results after each sample to avoid losing progress\n    if (i+1) % 10 == 0 or i == len(data_list) - 1:\n        save_to_json(data_list, filename=output_filename)\n\nprint(f\"\\nAll samples processed! Results saved to {output_filename}\")\nprint(f\"Task type used: {task_type}\")"
  },
  {
   "cell_type": "markdown",
   "id": "75ce611b-d8a9-4393-a9d8-66fdfdb3e299",
   "metadata": {},
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8cb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from judgerfunctions import *\n",
    "\n",
    "schemes = [\"KGW\", \"SWEET\", \"EXPEdit\",\"DIP\"]\n",
    "tasks = [\"HQA\", \"HQA2\", \"MEQS\"]\n",
    "model_type = \"meditron\"\n",
    "\n",
    "# load all json from directory that start with \"results_judged_\" to all_results\n",
    "all_json_files = sorted([f for f in os.listdir() if f.startswith(\"results_NEW2_judged_\") and f.endswith(\".json\")])\n",
    "\n",
    "all_json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b064c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}\n",
    "for scheme in schemes:\n",
    "    for task in tasks:\n",
    "        all_scores[scheme + \"-\" + task] = {\n",
    "            \"Watermarked\": np.array([0.0,0.0,0.0]),\n",
    "            \"Unwatermarked\": np.array([0.0,0.0,0.0]),\n",
    "            \"count_Watermarked\": 0,\n",
    "            \"count_Unwatermarked\": 0\n",
    "        }\n",
    "\n",
    "for scheme in schemes:\n",
    "    for task in tasks:\n",
    "        json_file = [f for f in all_json_files if scheme in f and task in f and ((task!=\"HQA\") or (task == \"HQA\" and \"HQA2\" not in f)) and model_type in f][0] # must be only one\n",
    "\n",
    "        print(f\"Loading {json_file}...\")\n",
    "\n",
    "        # Load the results from the JSON file\n",
    "        data_list = load_from_json(filename=json_file)\n",
    "\n",
    "        for i in range(len(data_list)):\n",
    "            item = data_list[i]\n",
    "            judge_choice = item[\"judge_choice\"]\n",
    "            if judge_choice != \"Watermarked\" and judge_choice != \"Unwatermarked\":\n",
    "                continue\n",
    "            all_scores[scheme + \"-\" + task][\"Watermarked\"] += np.array(item[\"scores_W\"])\n",
    "            all_scores[scheme + \"-\" + task][\"Unwatermarked\"] += np.array(item[\"scores_U\"])\n",
    "            all_scores[scheme + \"-\" + task][\"count_Watermarked\"] += 1\n",
    "            all_scores[scheme + \"-\" + task][\"count_Unwatermarked\"] += 1\n",
    "\n",
    "        # averaging\n",
    "        all_scores[scheme + \"-\" + task][\"Watermarked\"] = all_scores[scheme + \"-\" + task][\"Watermarked\"] / all_scores[scheme + \"-\" + task][\"count_Watermarked\"]\n",
    "        all_scores[scheme + \"-\" + task][\"Unwatermarked\"] = all_scores[scheme + \"-\" + task][\"Unwatermarked\"] / all_scores[scheme + \"-\" + task][\"count_Unwatermarked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Assuming all_scores is available with the same structure as in the original code\n",
    "data = all_scores  \n",
    "\n",
    "# Organize data by task\n",
    "tasks = ['HQA', 'HQA2', 'MEQS']\n",
    "schemes = ['KGW', 'SWEET', 'DIP','EXPEdit']\n",
    "schemes_name = ['KGW', 'SWEET', 'DiPmark','EXP-edit']\n",
    "\n",
    "# Set up color scheme for the categories\n",
    "category_colors = ['#fc8d59', '#a1c9f4', '#4c72b0']\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(6, 8), dpi=200)\n",
    "\n",
    "# Category names\n",
    "category_names = {\n",
    "    'HQA': ['Coherence', 'Relevance', 'Accuracy'],\n",
    "    'HQA2': ['Coherence', 'Relevance', 'Accuracy'],\n",
    "    'MEQS': ['Coherence', 'Completeness', 'Accuracy']\n",
    "}\n",
    "task_names = {\n",
    "    'HQA': 'Text Completion',\n",
    "    'HQA2': 'Question-Answering',\n",
    "    'MEQS': 'Summarization'\n",
    "}\n",
    "\n",
    "# Calculate percentage drops\n",
    "percentage_drops = {}\n",
    "for task in tasks:\n",
    "    for scheme in schemes:\n",
    "        key = f'{scheme}-{task}'\n",
    "        percentage_drops[key] = []\n",
    "        for i in range(3):  # Three aspects/categories\n",
    "            unwatermarked = data[key]['Unwatermarked'][i]\n",
    "            watermarked = data[key]['Watermarked'][i]\n",
    "            # Calculate percentage drop: (unwatermarked - watermarked) / unwatermarked * 100\n",
    "            if unwatermarked > 0:  # Avoid division by zero\n",
    "                drop = (unwatermarked - watermarked) / unwatermarked * 100\n",
    "            else:\n",
    "                drop = 0\n",
    "            percentage_drops[key].append(drop)\n",
    "\n",
    "# Create subplots for each task\n",
    "for task_idx, task in enumerate(tasks):\n",
    "    ax = plt.subplot(3, 1, task_idx + 1)\n",
    "    \n",
    "    x = np.arange(len(schemes))  # Position of scheme groups\n",
    "    width = 0.25  # Width of bars\n",
    "    \n",
    "    # Offsets for the three aspect bars\n",
    "    offsets = [-width, 0, width]\n",
    "    \n",
    "    # Create bars for each category\n",
    "    for i in range(3):  # Three categories/aspects\n",
    "        # Plot percentage drop bar for category i\n",
    "        plt.bar(x + offsets[i], \n",
    "                [percentage_drops[f'{scheme}-{task}'][i] for scheme in schemes],\n",
    "                width, \n",
    "                color=category_colors[i], \n",
    "                edgecolor='black',\n",
    "                label=category_names[task][i])\n",
    "    \n",
    "    plt.ylabel('Quality Drop (%)', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'{task_names[task]}', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(x, schemes_name, fontsize=12, fontweight='bold')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add percentage signs to y-axis\n",
    "    plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:.0f}%'))\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    plt.ylim(bottom=0)  # Start from 0%\n",
    "    \n",
    "    # Create legend for each subplot\n",
    "    legend_elements = []\n",
    "    \n",
    "    # Add category colors to legend\n",
    "    for i in range(3):\n",
    "        legend_elements.append(Patch(facecolor=category_colors[i], \n",
    "                                    edgecolor='black',\n",
    "                                    label=category_names[task][i]))\n",
    "    \n",
    "    # Add legend to the right of each subplot\n",
    "    ax.legend(handles=legend_elements, \n",
    "              loc='center left', \n",
    "              bbox_to_anchor=(1.01, 0.5),\n",
    "              fontsize=9)\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.subplots_adjust(hspace=0.4, right=0.85)  # Adjust right margin for legends\n",
    "# plt.suptitle('Percentage Quality Drop After Watermarking', \n",
    "            #  fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"KGW\", \"SWEET\",\"EXPEdit\",\"DIP\"]\n",
    "tasks = [\"MEQS\"]\n",
    "avg_methods_across_tasks = {}\n",
    "for method in methods:\n",
    "    tmp = []\n",
    "    for task in tasks:\n",
    "        key = f\"{method}-{task}\"\n",
    "        tmp.append(percentage_drops[key])\n",
    "    avg_methods_across_tasks[method] = np.mean(tmp, axis=0)\n",
    "# print pretty with only 2 decimal places for all float numbers\n",
    "for method in methods:\n",
    "    print(f\"{method}: {['{:.3f}'.format(x) for x in avg_methods_across_tasks[method]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e561c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}